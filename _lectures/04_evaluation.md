---
type: lecture
date: 2025-02-27T15:30:00+1:00
title: LLM agents and reasoning benchmarks
tldr: "We propose a framework over current evaluation benchmarks, and discuss potential research topics."
thumbnail: /static_files/presentations/agent_4.jpeg
hide_from_announcments: false
links: 
    - url: https://drive.google.com/file/d/1Ibnhgd44SF8C3N8uB6zS0zkwVZpLLCWl/view?usp=drive_link
      name: Basics
---
**Suggested Readings:**
- [SWE-bench: Can Language Models Resolve Real-World GitHub Issues?](https://arxiv.org/abs/2310.06770)
- [BabyAI: A Platform to Study the Sample Efficiency of Grounded Language Learning](https://arxiv.org/abs/1810.08272)
- [AgentGen: Enhancing Planning Abilities for Large Language Model based Agent via Environment and Task Generation](https://arxiv.org/abs/2408.00764)
- [FrontierMath: A Benchmark for Evaluating Advanced Mathematical Reasoning in AI](https://arxiv.org/abs/2411.04872)
- [Measuring Mathematical Problem Solving With the MATH Dataset](https://arxiv.org/abs/2103.03874)
- [AIME: AI System Optimization via Multiple LLM Evaluators](https://arxiv.org/abs/2410.03131)
- [AgentHarm: A Benchmark for Measuring Harmfulness of LLM Agents](https://arxiv.org/abs/2410.09024)
