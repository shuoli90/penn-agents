---
type: lecture
date: 2025-03-20T15:30:00+1:00
title: Safe and Trustworthy Agents
tldr: "We talk about trust taxonomy, attacks, and defenses."
thumbnail: /static_files/presentations/agent_5.jpeg
hide_from_announcments: false
links: 
    - url: https://drive.google.com/file/d/1PuMhO8qvUok5ufYF2A4X0bdUS1vtKKzG/view?usp=drive_link
      name: Basics
---
**Suggested Readings:**
- [Towards Trustworthy AI: A Review of Ethical and Robust Large Language Models](https://arxiv.org/html/2407.13934v1#:~:text=The%20rapid%20advancements%20in%20Large,Addressing%20these%20trust%20gaps%20is)
- [Personal LLM Agents: Insights and Survey about the Capability, Efficiency and Security](https://arxiv.org/pdf/2401.05459)
- [Commercial LLM Agents Are Already Vulnerable to Simple Yet Dangerous Attacks](https://arxiv.org/html/2502.08586v1#:~:text=Current%20research%20in%20LLM%20security,are%20integrated%20into%20agentic%20pipelines)
- [A Survey on Trustworthy LLM Agents: Threats and Countermeasures](https://arxiv.org/html/2503.09648v1#:~:text=emerged%20attacks%2C%20defenses%2C%20and%20evaluation,For%20easy)
- [DECODINGTRUST: A Comprehensive Assessment of Trustworthiness in GPT Models](https://arxiv.org/pdf/2306.11698)
- [BadAgent: Inserting and Activating Backdoor Attacks in LLM Agents](https://arxiv.org/pdf/2406.03007)
- [Unveiling Privacy Risks in LLM Agent Memory](https://arxiv.org/pdf/2502.13172)
- [Evil Geniuses: Delving into the Safety of LLM-based Agents](https://arxiv.org/pdf/2311.11855)
- [Extracting Training Data from Large Language Models](https://www.usenix.org/system/files/sec21-carlini-extracting.pdf#:~:text=This%20paper%20is%20included%20in,August%2011â€“13%2C%202021)
- [OpenFedLLM: Training Large Language Models on Decentralized Private Data via Federated Learning](https://arxiv.org/pdf/2402.06954)
- [StruQ: Defending Against Prompt Injection with Structured Queries](https://arxiv.org/pdf/2402.06363)
- [SmoothLLM: Defending Large Language Models Against Jailbreaking Attacks](https://arxiv.org/pdf/2310.03684)
- [GuardAgent: Safeguard LLM Agents via Knowledge-Enabled Reasoning](https://arxiv.org/pdf/2406.09187)
- [PrivacyLens: Evaluating Privacy Norm Awareness of Language Models in Action](https://arxiv.org/pdf/2409.00138)
- [A Path for Science and Evidence-based AI Policy](https://understanding-ai-safety.org)